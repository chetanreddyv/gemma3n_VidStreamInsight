{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal Video and Audio Analysis Pipeline\n",
    "This Jupyter Notebook demonstrates a complete pipeline for analyzing video frames and corresponding audio segments using Gemma-3n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Model Loading\n",
    "This cell imports all necessary libraries and loads the pre-trained Gemma model and its processor from Hugging Face. It also handles authentication using a Hugging Face token, which is assumed to be stored in Colab secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from google.colab import userdata  # For accessing Colab secrets\n",
    "\n",
    "# Define the model path\n",
    "MODEL_PATH = \"google/gemma-3n-E2B-it\"\n",
    "\n",
    "# Load the Hugging Face token from Colab secrets\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "\n",
    "# Set the Hugging Face token as an environment variable\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoProcessor.from_pretrained(MODEL_PATH, token=HF_TOKEN)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    ").eval().to(\"cuda\")  # Load the model to GPU for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define System Configuration\n",
    "Define key constants for the analysis pipeline, such as the target frames per second (FPS) for video sampling, the maximum number of frames to process, and the minimum duration for audio segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system configuration constants\n",
    "TARGET_FPS = 3  # Target frames per second for video sampling\n",
    "MAX_FRAMES = 30  # Maximum number of frames to process\n",
    "MIN_AUDIO_DURATION_S = 1.0  # Minimum duration for audio segments in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function: Extract Video Frames\n",
    "This cell contains the `extract_frames_from_video` function. It uses the `av` library to open a video file, decode it, and save individual frames at the specified `TARGET_FPS` rate up to `MAX_FRAMES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function: Extract Video Frames\n",
    "import tempfile\n",
    "import pathlib\n",
    "import av\n",
    "\n",
    "def extract_frames_from_video(video_path, target_fps=TARGET_FPS, max_frames=MAX_FRAMES):\n",
    "    \"\"\"\n",
    "    Extract frames from a video file at the specified FPS rate up to a maximum number of frames.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        target_fps (int): Target frames per second for sampling.\n",
    "        max_frames (int): Maximum number of frames to extract.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A list of tuples containing frame file paths and their corresponding timestamps, and the video duration.\n",
    "    \"\"\"\n",
    "    # Create a temporary directory to store extracted frames\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"frames_\")\n",
    "    container = av.open(video_path)  # Open the video file using the av library\n",
    "    video_stream = container.streams.video[0]  # Access the video stream\n",
    "    \n",
    "    # Calculate time-related parameters\n",
    "    time_base = video_stream.time_base\n",
    "    duration = float(video_stream.duration * time_base)\n",
    "    interval = 1.0 / target_fps\n",
    "    \n",
    "    # Determine the total number of frames to extract\n",
    "    total_frames = int(duration * target_fps)\n",
    "    if max_frames is not None:\n",
    "        total_frames = min(total_frames, max_frames)\n",
    "        \n",
    "    target_times = [i * interval for i in range(total_frames)]\n",
    "    target_index = 0\n",
    "    frame_paths = []\n",
    "    \n",
    "    # Decode video frames and save them at the specified intervals\n",
    "    for frame in container.decode(video=0):\n",
    "        if frame.pts is None:\n",
    "            continue\n",
    "            \n",
    "        timestamp = float(frame.pts * time_base)\n",
    "        \n",
    "        if target_index < len(target_times) and abs(timestamp - target_times[target_index]) < (interval / 2):\n",
    "            frame_path = pathlib.Path(temp_dir) / f\"frame_{target_index:04d}.jpg\"\n",
    "            frame_img = frame.to_image()\n",
    "            frame_img.save(frame_path)  # Save the frame as an image file\n",
    "            frame_paths.append((str(frame_path), target_index * interval))\n",
    "            target_index += 1\n",
    "            \n",
    "            if target_index >= max_frames:\n",
    "                break\n",
    "                \n",
    "    container.close()  # Close the video container\n",
    "    return frame_paths, duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function: Extract Audio Segments\n",
    "This cell defines the `extract_audio_segment` function. It uses the `pydub` library to load an audio file and extract a specific segment corresponding to a video frame's timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function: Extract Audio Segments\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def extract_audio_segment(audio_path, start_time, duration):\n",
    "    \"\"\"\n",
    "    Extract a segment of audio from the given file.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        start_time (float): Start time of the segment in seconds.\n",
    "        duration (float): Duration of the segment in seconds.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the temporary file containing the extracted audio segment.\n",
    "    \"\"\"\n",
    "    # Load the audio file using pydub\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    \n",
    "    # Extract the specified segment\n",
    "    segment = audio[start_time * 1000:(start_time + duration) * 1000]\n",
    "    \n",
    "    # Save the segment to a temporary file\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
    "    segment.export(temp_file.name, format=\"wav\")\n",
    "    \n",
    "    return temp_file.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Processing: Analyze Image and Audio\n",
    "The `process_inputs` function takes a single image and an audio file path. It prepares the inputs in the format required by the multimodal model, generates a textual description, and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Processing: Analyze Image and Audio\n",
    "def process_inputs(image, audio):\n",
    "    \"\"\"\n",
    "    Process a single image and audio segment using the Gemma 3n model.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): The image to be analyzed.\n",
    "        audio (str): Path to the audio file to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        str: The textual description generated by the model.\n",
    "    \"\"\"\n",
    "    # Prepare the input message in the required format\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"audio\", \"audio\": audio},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Apply the processor to prepare inputs for the model\n",
    "    input_ids = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_len = input_ids[\"input_ids\"].shape[-1]\n",
    "\n",
    "    # Move inputs to the model's device and set the appropriate data type\n",
    "    input_ids = input_ids.to(model.device, dtype=model.dtype)\n",
    "\n",
    "    # Perform inference using the model\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=256,\n",
    "            disable_compile=True\n",
    "        )\n",
    "\n",
    "    # Decode the generated output into text\n",
    "    text = processor.batch_decode(\n",
    "        outputs[:, input_len:],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    return text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Pipeline: Process the Entire Video\n",
    "The `process_video` function orchestrates the entire workflow. It calls the frame and audio extraction functions in a loop, passes each pair to the `process_inputs` function for analysis, and aggregates the results into a single report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def process_video(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Process an entire video by extracting frames and corresponding audio segments,\n",
    "    analyzing them using the Gemma 3n model, and aggregating the results.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        audio_path (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        str: Aggregated analysis results for the video.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract frames from the video\n",
    "        frame_paths, duration = extract_frames_from_video(video_path)\n",
    "        \n",
    "        if not frame_paths:\n",
    "            return \"Error: No frames extracted from video.\"\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        # Process each frame with its corresponding audio segment\n",
    "        for frame_path, timestamp in frame_paths:\n",
    "            # Calculate segment duration, ensuring it's not too short\n",
    "            inter_frame_duration = duration / len(frame_paths)\n",
    "            segment_duration = max(inter_frame_duration, MIN_AUDIO_DURATION_S)\n",
    "            \n",
    "            # Extract audio for this segment\n",
    "            audio_segment = extract_audio_segment(audio_path, timestamp, segment_duration)\n",
    "            \n",
    "            # Load the frame as a PIL Image\n",
    "            image = Image.open(frame_path)\n",
    "            \n",
    "            # Process the frame and audio segment\n",
    "            result = process_inputs(image, audio_segment)\n",
    "            \n",
    "            # Format the result with a timestamp\n",
    "            time_str = f\"[{timestamp:.1f}s - {timestamp + segment_duration:.1f}s]\"\n",
    "            all_results.append(f\"{time_str}: {result}\")\n",
    "            \n",
    "            # Clean up the temporary audio segment file\n",
    "            os.unlink(audio_segment)\n",
    "        \n",
    "        # Clean up the temporary frame files\n",
    "        for path, _ in frame_paths:\n",
    "            if os.path.exists(path):\n",
    "                os.unlink(path)\n",
    "        \n",
    "        # Return the aggregated results\n",
    "        return \"\\n\\n\".join(all_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing video: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch the Interactive Web Interface\n",
    "This cell uses `gradio` to create a simple web UI. The interface allows a user to upload a video and an audio file, which are then processed by the `process_video` function. The final analysis is displayed in a textbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio interface for interactive video and audio analysis\n",
    "import gradio as gr\n",
    "\n",
    "# Define the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_video,  # Function to process the video and audio\n",
    "    inputs=[\n",
    "        gr.Video(label=\"Upload Video\"),  # Input for video file\n",
    "        gr.Audio(label=\"Upload Audio\", type=\"filepath\")  # Input for audio file\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Analysis Results\"),  # Output textbox for results\n",
    "    title=\"Video Stream Analysis with Audio\",  # Title of the interface\n",
    "    description=\"Upload a video file and its audio. The system processes the video frames and analyzes them with the Gemma 3n model.\"  # Description of the interface\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
